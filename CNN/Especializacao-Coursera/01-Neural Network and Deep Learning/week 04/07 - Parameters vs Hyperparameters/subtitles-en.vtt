WEBVTT

1
00:00:00.060 --> 00:00:02.669
Being effective in developing your deep

2
00:00:02.669 --> 00:00:04.380
Neural Nets requires that you not only

3
00:00:04.380 --> 00:00:06.870
organize your parameters well but also

4
00:00:06.870 --> 00:00:09.269
your hyper parameters. So what are hyper

5
00:00:09.269 --> 00:00:11.759
parameters? let's take a look! So the

6
00:00:11.759 --> 00:00:15.170
parameters your model are W and B and

7
00:00:15.170 --> 00:00:17.820
there are other things you need to tell

8
00:00:17.820 --> 00:00:21.720
your learning algorithm, such as the

9
00:00:21.720 --> 00:00:26.220
learning rate alpha, because we need

10
00:00:26.220 --> 00:00:28.920
to set alpha and that in turn will

11
00:00:28.920 --> 00:00:32.329
determine how your parameters evolve or

12
00:00:32.329 --> 00:00:34.890
maybe the number of iterations of

13
00:00:34.890 --> 00:00:38.190
gradient descent you carry out. Your

14
00:00:38.190 --> 00:00:40.170
learning algorithm has oth

15
00:00:40.170 --> 00:00:42.629
numbers that you need to set such as the

16
00:00:42.629 --> 00:00:47.340
number of hidden layers, so we call that

17
00:00:47.340 --> 00:00:50.629
capital L, or the number of hidden units,

18
00:00:50.629 --> 00:00:56.039
such as 0 and 1 and 2 and

19
00:00:56.039 --> 00:00:59.670
so on. Then you also have the choice

20
00:00:59.670 --> 00:01:03.329
of activation function. do you want to

21
00:01:03.329 --> 00:01:05.610
use a RELU, or tangent or a sigmoid

22
00:01:05.610 --> 00:01:06.869
function especially in the

23
00:01:06.869 --> 00:01:11.760
hidden layers. So all of these things

24
00:01:11.760 --> 00:01:13.590
are things that you need to tell your

25
00:01:13.590 --> 00:01:15.990
learning algorithm and so these are

26
00:01:15.990 --> 00:01:19.640
parameters that control the ultimate

27
00:01:19.640 --> 00:01:22.200
parameters W and B and so we call all of

28
00:01:22.200 --> 00:01:25.640
these things below hyper parameters.

29
00:01:25.640 --> 00:01:29.340
Because these things like alpha, the

30
00:01:29.340 --> 00:01:30.750
learning rate, the number of iterations,

31
00:01:30.750 --> 00:01:32.369
number of hidden layers, and so on, these

32
00:01:32.369 --> 00:01:36.000
are all parameters that control W and B.

33
00:01:36.000 --> 00:01:39.290
So we call these things hyper parameters,

34
00:01:39.290 --> 00:01:41.970
because it is the hyper parameters that

35
00:01:41.970 --> 00:01:44.250
somehow determine the final

36
00:01:44.250 --> 00:01:46.950
value of the parameters W and B that you

37
00:01:46.950 --> 00:01:50.100
end up with. In fact, deep learning has a

38
00:01:50.100 --> 00:01:53.520
lot of different hyper parameters.

39
00:01:53.520 --> 00:01:55.470
In the later course, we'll see other

40
00:01:55.470 --> 00:01:57.899
hyper parameters as well such as the

41
00:01:57.899 --> 00:02:05.150
momentum term, the mini batch size,

42
00:02:05.150 --> 00:02:07.220
various forms of regularization

43
00:02:07.220 --> 00:02:13.020
parameters, and so on. If none of

44
00:02:13.020 --> 00:02:14.700
these terms at the bottom make sense yet,

45
00:02:14.700 --> 00:02:16.020
don't worry about it! We'll talk about

46
00:02:16.020 --> 00:02:18.810
them in the second course. Because deep

47
00:02:18.810 --> 00:02:21.870
learning has so many hyper parameters in

48
00:02:21.870 --> 00:02:24.120
contrast to earlier errors of machine

49
00:02:24.120 --> 00:02:26.370
learning, I'm going to try to be very

50
00:02:26.370 --> 00:02:28.890
consistent in calling the learning rate

51
00:02:28.890 --> 00:02:31.050
alpha a hyper parameter rather than

52
00:02:31.050 --> 00:02:33.480
calling the parameter. I think in earlier

53
00:02:33.480 --> 00:02:35.250
eras of machine learning when we didn't

54
00:02:35.250 --> 00:02:37.920
have so many hyper parameters, most of us

55
00:02:37.920 --> 00:02:39.600
used to be a bit slow up here and just

56
00:02:39.600 --> 00:02:42.120
call alpha a parameter. Technically,

57
00:02:42.120 --> 00:02:44.610
alpha is a parameter, but is a parameter

58
00:02:44.610 --> 00:02:47.580
that determines the real parameters. I'll

59
00:02:47.580 --> 00:02:50.280
try to be consistent in calling these

60
00:02:50.280 --> 00:02:51.570
things like alpha, the number of

61
00:02:51.570 --> 00:02:54.180
iterations, and so on hyper parameters. So

62
00:02:54.180 --> 00:02:55.769
when you're training a deep net for your

63
00:02:55.769 --> 00:02:57.810
own application you find that there may

64
00:02:57.810 --> 00:02:59.940
be a lot of possible settings for the

65
00:02:59.940 --> 00:03:01.560
hyper parameters that you need to just

66
00:03:01.560 --> 00:03:04.440
try out. So applying deep learning today is

67
00:03:04.440 --> 00:03:07.230
a very intrictate process where often you

68
00:03:07.230 --> 00:03:09.840
might have an idea. For example, you might

69
00:03:09.840 --> 00:03:12.150
have an idea for the best value for the

70
00:03:12.150 --> 00:03:13.549
learning rate. You might say, well maybe

71
00:03:13.549 --> 00:03:16.739
alpha equals 0.01 I want to try that.

72
00:03:16.739 --> 00:03:20.670
Then you implement, try it out, and then

73
00:03:20.670 --> 00:03:22.530
see how that works. Based on

74
00:03:22.530 --> 00:03:23.910
that outcome you might say, you know what?

75
00:03:23.910 --> 00:03:25.890
I've changed online, I want to increase

76
00:03:25.890 --> 00:03:28.620
the learning rate to 0.05. So, if

77
00:03:28.620 --> 00:03:30.930
you're not sure what the best value

78
00:03:30.930 --> 00:03:32.970
for the learning rate to use. You might

79
00:03:32.970 --> 00:03:35.010
try one value of the learning rate alpha

80
00:03:35.010 --> 00:03:37.680
and see their cost function j go down

81
00:03:37.680 --> 00:03:39.690
like this, then you might try a larger

82
00:03:39.690 --> 00:03:41.820
value for the learning rate alpha and

83
00:03:41.820 --> 00:03:43.650
see the cost function blow up and

84
00:03:43.650 --> 00:03:45.060
diverge. Then, you might try another

85
00:03:45.060 --> 00:03:47.250
version and see it go down really fast.

86
00:03:47.250 --> 00:03:49.709
it's inverse to higher value. You might

87
00:03:49.709 --> 00:03:51.780
try another version and

88
00:03:51.780 --> 00:03:53.670
see the cost function J do that then.

89
00:03:53.670 --> 00:03:55.530
I'll be trying to set the values. So you might

90
00:03:55.530 --> 00:03:57.840
say, okay looks like this the value of

91
00:03:57.840 --> 00:04:00.870
alpha. It gives me a pretty fast learning

92
00:04:00.870 --> 00:04:02.790
and allows me to converge to a lower

93
00:04:02.790 --> 00:04:04.290
cost function j and so I'm going to use

94
00:04:04.290 --> 00:04:06.720
this value of alpha. You saw in a

95
00:04:06.720 --> 00:04:08.040
previous slide that there are a lot of

96
00:04:08.040 --> 00:04:10.170
different hybrid parameters. It turns

97
00:04:10.170 --> 00:04:11.489
out that when you're starting on the new

98
00:04:11.489 --> 00:04:13.830
application, you should find it very

99
00:04:13.830 --> 00:04:15.450
difficult to know in advance exactly

100
00:04:15.450 --> 00:04:17.940
what is the best value of the hyper

101
00:04:17.940 --> 00:04:20.580
parameters. So, what often happens is you

102
00:04:20.580 --> 00:04:22.169
just have to try out many different

103
00:04:22.169 --> 00:04:24.570
values and go around this cycle your

104
00:04:24.570 --> 00:04:26.970
try out some values, really try five hidden

105
00:04:26.970 --> 00:04:28.440
layers. With this many number of hidden

106
00:04:28.440 --> 00:04:31.140
units implement that, see if it works, and

107
00:04:31.140 --> 00:04:34.140
then iterate. So the title of this slide

108
00:04:34.140 --> 00:04:36.180
is that applying deep learning is a very

109
00:04:36.180 --> 00:04:38.340
empirical process, and empirical process

110
00:04:38.340 --> 00:04:40.740
is maybe a fancy way of saying you just

111
00:04:40.740 --> 00:04:42.419
have to try a lot of things and see what

112
00:04:42.419 --> 00:04:45.330
works. Another effect I've seen is that

113
00:04:45.330 --> 00:04:47.190
deep learning today is applied to so

114
00:04:47.190 --> 00:04:48.810
many problems ranging from computer

115
00:04:48.810 --> 00:04:51.990
vision, to speech recognition, to natural

116
00:04:51.990 --> 00:04:53.789
language processing, to a lot of

117
00:04:53.789 --> 00:04:55.500
structured data applications such as

118
00:04:55.500 --> 00:04:59.250
maybe a online advertising, or web search,

119
00:04:59.250 --> 00:05:02.430
or product recommendations, and so on.

120
00:05:02.430 --> 00:05:05.640
What I've seen is that first, I've seen

121
00:05:05.640 --> 00:05:08.190
researchers from one discipline, any one

122
00:05:08.190 --> 00:05:10.080
of these, and try to go to a different one.

123
00:05:10.080 --> 00:05:12.060
And sometimes the intuitions about hyper

124
00:05:12.060 --> 00:05:14.400
parameters carries over and sometimes it

125
00:05:14.400 --> 00:05:16.590
doesn't, so I often advise people,

126
00:05:16.590 --> 00:05:17.849
especially when starting on a new

127
00:05:17.849 --> 00:05:20.970
problem, to just try out a range of

128
00:05:20.970 --> 00:05:23.550
values and see what w. In the next

129
00:05:23.550 --> 00:05:25.500
course we'll

130
00:05:25.500 --> 00:05:27.930
see some systematic ways for trying out

131
00:05:27.930 --> 00:05:30.780
a range of values. Second,

132
00:05:30.780 --> 00:05:32.070
even if you're working on one

133
00:05:32.070 --> 00:05:33.570
application for a long time, you know

134
00:05:33.570 --> 00:05:35.220
maybe you're working on online

135
00:05:35.220 --> 00:05:37.979
advertising, as you make progress on the

136
00:05:37.979 --> 00:05:39.930
problem it is quite possible that the best

137
00:05:39.930 --> 00:05:41.580
value for the learning rate, a number of

138
00:05:41.580 --> 00:05:43.830
hidden units, and so on might change. So

139
00:05:43.830 --> 00:05:46.440
even if you tune your system to the best

140
00:05:46.440 --> 00:05:49.229
value of hyper parameters today it's

141
00:05:49.229 --> 00:05:51.750
possible you'll find that the best value

142
00:05:51.750 --> 00:05:53.430
might change a year from now maybe

143
00:05:53.430 --> 00:05:55.650
because the computer infrastructure,

144
00:05:55.650 --> 00:05:57.840
be it you know CPUs, or the type of GPU

145
00:05:57.840 --> 00:05:59.789
running on, or something has changed.

146
00:05:59.789 --> 00:06:01.560
So maybe one rule of thumb is

147
00:06:01.560 --> 00:06:03.659
every now and then, maybe every few

148
00:06:03.659 --> 00:06:05.070
months, if you're working on a problem

149
00:06:05.070 --> 00:06:06.659
for an extended period of time for many

150
00:06:06.659 --> 00:06:09.030
years just try a few values for the

151
00:06:09.030 --> 00:06:10.800
hyper parameters and double check if

152
00:06:10.800 --> 00:06:12.570
there's a better value for the hyper

153
00:06:12.570 --> 00:06:15.150
parameters. As you do so you slowly

154
00:06:15.150 --> 00:06:17.280
gain intuition as well about the hyper

155
00:06:17.280 --> 00:06:18.779
parameters that work best for your

156
00:06:18.779 --> 00:06:19.870
problems.

157
00:06:19.870 --> 00:06:21.820
I know that this might seem like an

158
00:06:21.820 --> 00:06:24.010
unsatisfying part of deep learning that

159
00:06:24.010 --> 00:06:25.510
you just have to try on all the values

160
00:06:25.510 --> 00:06:27.940
for these hyper parameters, but maybe

161
00:06:27.940 --> 00:06:30.160
this is one area where deep learning

162
00:06:30.160 --> 00:06:32.200
research is still advancing, and maybe

163
00:06:32.200 --> 00:06:33.850
over time we'll be able to give better

164
00:06:33.850 --> 00:06:36.190
guidance for the best hyper parameters

165
00:06:36.190 --> 00:06:38.350
to use. It's also possible that

166
00:06:38.350 --> 00:06:41.260
because CPUs and GPUs and networks and

167
00:06:41.260 --> 00:06:43.630
data sets are all changing, and it is

168
00:06:43.630 --> 00:06:45.910
possible that the guidance won't

169
00:06:45.910 --> 00:06:47.680
converge for some time. You just need

170
00:06:47.680 --> 00:06:49.360
to keep trying out different values and

171
00:06:49.360 --> 00:06:50.860
evaluate them on a hold on

172
00:06:50.860 --> 00:06:52.479
cross-validation set or something and

173
00:06:52.479 --> 00:06:54.100
pick the value that works for your

174
00:06:54.100 --> 00:06:56.350
problems. So that was a brief discussion

175
00:06:56.350 --> 00:06:58.870
of hyper parameters. In the second course,

176
00:06:58.870 --> 00:07:01.030
we'll also give some suggestions for how

177
00:07:01.030 --> 00:07:03.280
to systematically explore the space of

178
00:07:03.280 --> 00:07:06.040
hyper parameters but by now you actually

179
00:07:06.040 --> 00:07:07.570
have pretty much all the tools you need

180
00:07:07.570 --> 00:07:09.430
to do their programming exercise before

181
00:07:09.430 --> 00:07:11.470
you do that adjust or share view one

182
00:07:11.470 --> 00:07:14.050
more set of ideas which is I often ask

183
00:07:14.050 --> 00:07:16.150
what does deep learning have to do the

184
00:07:16.150 --> 00:07:18.660
human brain?