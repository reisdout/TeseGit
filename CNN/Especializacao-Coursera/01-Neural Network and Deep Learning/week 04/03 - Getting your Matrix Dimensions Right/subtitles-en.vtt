WEBVTT

1
00:00:00.000 --> 00:00:01.965
When implementing a
deep neural network,

2
00:00:01.965 --> 00:00:04.200
one of the debugging
tools I often

3
00:00:04.200 --> 00:00:06.555
use to check the
correctness of my code

4
00:00:06.555 --> 00:00:08.760
is to pull a piece
of paper and just

5
00:00:08.760 --> 00:00:11.475
work through the dimensions
in matrix I'm working with.

6
00:00:11.475 --> 00:00:14.190
Let me show you how to do
that since I hope this

7
00:00:14.190 --> 00:00:15.810
will make it easier for you

8
00:00:15.810 --> 00:00:17.490
to implement your deep
networks as well.

9
00:00:17.490 --> 00:00:20.300
So capital L is equal to 5.

10
00:00:20.300 --> 00:00:21.485
I counted them quickly.

11
00:00:21.485 --> 00:00:22.820
Not counting the input layer,

12
00:00:22.820 --> 00:00:25.130
there are five layers here,

13
00:00:25.130 --> 00:00:27.590
four hidden layers
and one output layer.

14
00:00:27.590 --> 00:00:29.840
If you implement
forward propagation,

15
00:00:29.840 --> 00:00:34.845
the first step will be Z1 equals

16
00:00:34.845 --> 00:00:41.655
W1 times the input
features x plus b1.

17
00:00:41.655 --> 00:00:45.230
Let's ignore the bias terms B

18
00:00:45.230 --> 00:00:48.425
for now and focus on
the parameters W. Now,

19
00:00:48.425 --> 00:00:52.725
this first hidden layer
has three hidden units.

20
00:00:52.725 --> 00:00:56.000
So this is Layer 0,

21
00:00:56.000 --> 00:00:58.015
Layer 1, Layer 2, Layer 3,

22
00:00:58.015 --> 00:00:59.685
Layer 4, and Layer 5.

23
00:00:59.685 --> 00:01:02.960
Using the notation we had
from the previous video,

24
00:01:02.960 --> 00:01:04.610
we have that n1, which is

25
00:01:04.610 --> 00:01:06.890
the number of hidden
units in layer 1,

26
00:01:06.890 --> 00:01:08.705
is equal to 3.

27
00:01:08.705 --> 00:01:13.485
Here we would have
that n2 is equal to 5,

28
00:01:13.485 --> 00:01:16.620
n3 is equal to 4,

29
00:01:16.620 --> 00:01:19.440
n4 is equal to 2,

30
00:01:19.440 --> 00:01:23.070
and n5 is equal to 1.

31
00:01:23.070 --> 00:01:24.730
So far we've only seen

32
00:01:24.730 --> 00:01:27.010
neural networks with
a single output unit,

33
00:01:27.010 --> 00:01:29.380
but in later courses
we'll talk about

34
00:01:29.380 --> 00:01:32.425
neural networks with multiple
output units as well.

35
00:01:32.425 --> 00:01:35.665
Finally, for the input layer,

36
00:01:35.665 --> 00:01:40.485
we also have n0 equals
nX is equal to 2.

37
00:01:40.485 --> 00:01:45.775
Now, let's think about the
dimensions of Z, W, and X.

38
00:01:45.775 --> 00:01:48.205
Z is the vector of activations

39
00:01:48.205 --> 00:01:51.090
for this first hidden layer.

40
00:01:51.090 --> 00:01:56.080
So Z is going to be 3 by 1,

41
00:01:56.080 --> 00:01:59.060
is going to be a
three-dimensional vector.

42
00:01:59.060 --> 00:02:06.255
I'm going to write it as, n1
by one-dimensional matrix,

43
00:02:06.255 --> 00:02:08.045
3 by 1 in this case.

44
00:02:08.045 --> 00:02:10.460
Now, how about the
input features x?

45
00:02:10.460 --> 00:02:12.215
X we have two input features.

46
00:02:12.215 --> 00:02:14.135
So x is, in this example,

47
00:02:14.135 --> 00:02:18.835
2 by 1, but more generally
it'll be n0 by 1.

48
00:02:18.835 --> 00:02:22.410
What we need is for the
matrix W1 to be something

49
00:02:22.410 --> 00:02:26.445
that when we multiply an
n0 by 1 vector to it,

50
00:02:26.445 --> 00:02:29.565
we get an n1 by 1 vector.

51
00:02:29.565 --> 00:02:34.625
So you have a
three-dimensional vector

52
00:02:34.625 --> 00:02:38.690
equals something times a
two-dimensional vector.

53
00:02:38.690 --> 00:02:41.990
By the rules of matrix
multiplication,

54
00:02:41.990 --> 00:02:46.800
this has got to be
a 3 by 2 matrix.

55
00:02:47.540 --> 00:02:50.880
Because a 3 by 2
matrix times a 2 by

56
00:02:50.880 --> 00:02:53.640
1 matrix or times
a 2 by 1 vector,

57
00:02:53.640 --> 00:02:56.190
that gives you a 3 by 1 vector.

58
00:02:56.190 --> 00:02:58.450
More generally,
this is going to be

59
00:02:58.450 --> 00:03:02.540
an n1 by n0 dimensional matrix.

60
00:03:02.540 --> 00:03:06.815
So what we figured out here
is that the dimensions of W1

61
00:03:06.815 --> 00:03:12.465
has to be n1 by n0,

62
00:03:12.465 --> 00:03:15.860
and more generally,
the dimensions of

63
00:03:15.860 --> 00:03:20.360
WL must be nL by nL minus 1.

64
00:03:20.360 --> 00:03:23.540
For example, the
dimensions of W2,

65
00:03:23.540 --> 00:03:29.075
for this, it will
have to be 5 by 3,

66
00:03:29.075 --> 00:03:35.080
or it will be n2 by n1,

67
00:03:35.080 --> 00:03:39.100
because we're going
to compute Z2

68
00:03:39.100 --> 00:03:46.875
as W2 times a1.

69
00:03:46.875 --> 00:03:50.535
Again, let's ignore
the bias for now.

70
00:03:50.535 --> 00:03:54.420
This is going to be 3 by 1.

71
00:03:54.420 --> 00:03:57.780
We need this to be 5 by 1.

72
00:03:57.780 --> 00:04:02.625
So this had better be 5 by 3.

73
00:04:02.625 --> 00:04:10.995
Similarly, W3 is really the
dimension of the next layer,

74
00:04:10.995 --> 00:04:13.605
the dimension of
the previous layer.

75
00:04:13.605 --> 00:04:16.620
So this is going to be 4 by 5.

76
00:04:16.620 --> 00:04:22.830
W4 is going to

77
00:04:22.830 --> 00:04:27.765
be 2 by 4,

78
00:04:27.765 --> 00:04:33.790
and W5 is going to be 1 by 2.

79
00:04:33.860 --> 00:04:37.240
The general formula to
check is that when you're

80
00:04:37.240 --> 00:04:40.270
implementing the
matrix for a layer L,

81
00:04:40.270 --> 00:04:48.030
that the dimension of that
matrix be nL by nL minus 1.

82
00:04:48.030 --> 00:04:53.320
Now, let's think about the
dimension of this vector B.

83
00:04:53.320 --> 00:04:58.590
This is going to be
a 3 by 1 vector,

84
00:04:58.590 --> 00:05:01.840
so you have to add
that to another 3 by

85
00:05:01.840 --> 00:05:07.340
1 vector in order to get a 3
by 1 vector as the output.

86
00:05:08.500 --> 00:05:11.735
This was going to be 5 by 1,

87
00:05:11.735 --> 00:05:14.810
so there's going to be
another 5 by 1 vector

88
00:05:14.810 --> 00:05:16.700
in order for the sum

89
00:05:16.700 --> 00:05:18.860
of these two things that
I have in the boxes to

90
00:05:18.860 --> 00:05:22.350
be itself a 5 by 1 vector.

91
00:05:22.350 --> 00:05:26.380
The more general rule is that
in the example on the left,

92
00:05:26.380 --> 00:05:31.180
b^[1] is n^[1] by 1,

93
00:05:31.180 --> 00:05:33.325
like this 3 by 1.

94
00:05:33.325 --> 00:05:35.214
In the second example,

95
00:05:35.214 --> 00:05:42.400
it is this is n^[2] by 1 and so

96
00:05:42.400 --> 00:05:44.440
the more general
case is that b^[l]

97
00:05:44.440 --> 00:05:50.605
should be n^[l]
by 1 dimensional.

98
00:05:50.605 --> 00:05:53.620
Hopefully, these two
equations help you to

99
00:05:53.620 --> 00:05:56.455
double-check that the
dimensions of your matrices,

100
00:05:56.455 --> 00:05:58.900
w, as well as of

101
00:05:58.900 --> 00:06:01.885
your vectors b are the
correct dimensions.

102
00:06:01.885 --> 00:06:05.245
Of course, if you're
implementing back-propagation,

103
00:06:05.245 --> 00:06:06.925
then the dimensions of

104
00:06:06.925 --> 00:06:10.075
dw should be the
same as dimension of

105
00:06:10.075 --> 00:06:15.970
w. So dw should be the
same dimension as w,

106
00:06:15.970 --> 00:06:21.835
and db should be the
same dimension as b.

107
00:06:21.835 --> 00:06:24.519
Now, the other key
set of quantities

108
00:06:24.519 --> 00:06:28.015
whose dimensions to
check are these z,

109
00:06:28.015 --> 00:06:31.315
x, as well as a of l,

110
00:06:31.315 --> 00:06:33.550
which we didn't talk
too much about here.

111
00:06:33.550 --> 00:06:39.880
But because z of l is
equal to g of a of l,

112
00:06:39.880 --> 00:06:42.655
apply element-wise then z and

113
00:06:42.655 --> 00:06:44.695
a should have the same dimension

114
00:06:44.695 --> 00:06:46.540
in these types of networks.

115
00:06:46.540 --> 00:06:48.865
Now, let's see what
happens when you have

116
00:06:48.865 --> 00:06:50.770
a vectorized implementation that

117
00:06:50.770 --> 00:06:52.930
looks at multiple
examples at a time.

118
00:06:52.930 --> 00:06:55.780
Even for a vectorized
implementation, of course,

119
00:06:55.780 --> 00:06:57.940
the dimensions of w,

120
00:06:57.940 --> 00:07:00.685
b, dw, and db will
stay the same.

121
00:07:00.685 --> 00:07:03.895
But the dimensions of za,

122
00:07:03.895 --> 00:07:06.370
as well as x, will change a bit

123
00:07:06.370 --> 00:07:09.355
in your vectorized
implementation.

124
00:07:09.355 --> 00:07:14.140
Previously we had z^[1] equals

125
00:07:14.140 --> 00:07:21.910
w^[1] times x plus b^]1],

126
00:07:21.910 --> 00:07:26.050
where this was n^[1] by 1.

127
00:07:26.050 --> 00:07:29.660
This was n^[1] by n^[0],

128
00:07:29.660 --> 00:07:36.250
x was n^[0] by 1,

129
00:07:36.250 --> 00:07:40.720
and b was n^[1] by 1.

130
00:07:40.720 --> 00:07:43.555
Now, in a vectorized
implementation,

131
00:07:43.555 --> 00:07:53.640
you would have z^[1] equals
w^[1] times x plus b^[1].

132
00:07:53.640 --> 00:07:55.950
Where now z^[1] is obtained by

133
00:07:55.950 --> 00:07:59.770
taking the z^[1] for the
individual examples.

134
00:07:59.770 --> 00:08:05.170
So there's z^[1][1],
z^[1][2] up to z^[1][m] and

135
00:08:05.170 --> 00:08:10.660
stacking them as follows
and this gives you z^[1].

136
00:08:10.660 --> 00:08:12.580
The dimension of z^[1] is that

137
00:08:12.580 --> 00:08:15.025
instead of being n^[1] by 1,

138
00:08:15.025 --> 00:08:17.740
it ends up being n^[1] by m,

139
00:08:17.740 --> 00:08:19.990
if m is decisive training set.

140
00:08:19.990 --> 00:08:23.200
The dimensions of
w^[1] stays the same

141
00:08:23.200 --> 00:08:26.605
so is the n^[1] by n^[0] and

142
00:08:26.605 --> 00:08:29.500
x instead of being n^[0] by

143
00:08:29.500 --> 00:08:33.490
1 is now all your training
examples stamped horizontally,

144
00:08:33.490 --> 00:08:38.500
so it's now n^[0] by m. You
notice that when you take a,

145
00:08:38.500 --> 00:08:40.930
n^[1] by n^[0] matrics and

146
00:08:40.930 --> 00:08:43.810
multiply that by an
n^[0] by m matrics

147
00:08:43.810 --> 00:08:46.390
that together they
actually give you an

148
00:08:46.390 --> 00:08:49.930
n^[1] by m dimensional
matrics as expected.

149
00:08:49.930 --> 00:08:56.215
Now the final detail is that
b^[1] is still n^[1] by 1.

150
00:08:56.215 --> 00:08:58.855
But when you take
this and add it to b,

151
00:08:58.855 --> 00:09:02.875
then through python broadcasting
this will get duplicated

152
00:09:02.875 --> 00:09:08.035
into an n^[1] by m matrics
and then added element-wise.

153
00:09:08.035 --> 00:09:09.670
On the previous slide,

154
00:09:09.670 --> 00:09:12.160
we talked about the
dimensions of w,

155
00:09:12.160 --> 00:09:14.770
b, dw, and db.

156
00:09:14.770 --> 00:09:18.040
Here what we see is
that whereas z^[l],

157
00:09:18.040 --> 00:09:22.540
as well as a^[l],

158
00:09:22.540 --> 00:09:27.340
are of dimension n^[l] by 1,

159
00:09:27.340 --> 00:09:31.480
we have now instead
that capital Z^[l],

160
00:09:31.480 --> 00:09:36.190
as well as capital A^[l],

161
00:09:36.190 --> 00:09:38.230
are n^[l] by m.

162
00:09:38.230 --> 00:09:42.354
A special case of this
is when l is equal to 0,

163
00:09:42.354 --> 00:09:45.280
in which case A^[0],

164
00:09:45.280 --> 00:09:47.230
which is equal to
just your training

165
00:09:47.230 --> 00:09:49.540
set input features x is going

166
00:09:49.540 --> 00:09:54.295
to be equal to n^[0]
by m as expected.

167
00:09:54.295 --> 00:09:56.200
Of course, when you're

168
00:09:56.200 --> 00:10:02.020
implementing this in
back-propagation,

169
00:10:02.020 --> 00:10:09.205
we'll see later you end up
computing dz as well as da.

170
00:10:09.205 --> 00:10:15.315
This way, of course, has the
same dimension as z and a.

171
00:10:15.315 --> 00:10:18.330
Hope the low exercise
went through helps

172
00:10:18.330 --> 00:10:19.890
clarify the dimensions of

173
00:10:19.890 --> 00:10:21.885
the various matrices
you'll be working with.

174
00:10:21.885 --> 00:10:23.850
When you implement
back-propagation

175
00:10:23.850 --> 00:10:25.005
for a deep neural network,

176
00:10:25.005 --> 00:10:27.420
so long as you work through
your code and make sure that

177
00:10:27.420 --> 00:10:30.225
all the matrices or
dimensions are consistent,

178
00:10:30.225 --> 00:10:32.760
that will usually
help you go some ways

179
00:10:32.760 --> 00:10:35.800
towards eliminating some
class of possible bugs.

180
00:10:35.800 --> 00:10:38.260
I hope that exercise
for figuring out

181
00:10:38.260 --> 00:10:40.090
the dimensions of
the various matrices

182
00:10:40.090 --> 00:10:41.980
you'd be working
with is helpful.

183
00:10:41.980 --> 00:10:43.450
When you implement a deep neural

184
00:10:43.450 --> 00:10:45.080
network if you keep straight

185
00:10:45.080 --> 00:10:46.790
the dimensions of
these various matrices

186
00:10:46.790 --> 00:10:48.095
and vectors you're working with,

187
00:10:48.095 --> 00:10:49.835
hopefully, that will
help you eliminate

188
00:10:49.835 --> 00:10:51.575
some class of possible bugs.

189
00:10:51.575 --> 00:10:54.525
It certainly helps me
get my code right.

190
00:10:54.525 --> 00:10:58.040
Next, we've now seen
some of the mechanics of

191
00:10:58.040 --> 00:11:01.280
how to do the forward
propagation in a neural network.

192
00:11:01.280 --> 00:11:04.250
But why are deep neural
networks so effective and

193
00:11:04.250 --> 00:11:07.295
why do they do better than
shallow representations?

194
00:11:07.295 --> 00:11:10.770
Let's spend a few minutes in
the next video to discuss.